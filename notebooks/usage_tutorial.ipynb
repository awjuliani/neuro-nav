{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuro-Nav Usage Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment below lines and run this cell to install neuronav and requirements for colab.\n",
    "\n",
    "#!git clone https://github.com/awjuliani/neuro-nav\n",
    "#!pip install ./neuro-nav[experiments_remote]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuronav.envs.grid_env import GridEnv, GridSize, GridObsType, OrientationType\n",
    "from neuronav.envs.graph_env import GraphEnv\n",
    "from neuronav.agents.td_agents import TDSR, TDQ, SARSA\n",
    "from neuronav.envs.grid_topographies import GridTopography\n",
    "from neuronav.envs.graph_structures import GraphStructure\n",
    "from neuronav.utils import run_episode\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the `GridEnv`\n",
    "\n",
    "For information on supported environment topographies and observation spaces, see [here](https://github.com/awjuliani/neuro-nav/tree/main/neuronav/envs#gridenv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridEnv()\n",
    "obs = env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GridEnv` follows the OpenAI Gym interface API. We can act in the environment by calling `env.step(action)`. Here we pass action `0` which corresponds to moving a step upward in the grid environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, reward, done, _ = env.step(0)\n",
    "print(f\"Reward: {reward}\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a different grid topography\n",
    "\n",
    "Topographies can be selected by passing a `GridTopography` enum as an argument when creating the `GridEnv` object. Here we create the classic four rooms environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridEnv(topography=GridTopography.four_rooms)\n",
    "obs = env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to select either the `small` or `large` variant of each topography using the `GridSize` enum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridEnv(topography=GridTopography.four_rooms, grid_size=GridSize.large)\n",
    "obs = env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a different observation type\n",
    "\n",
    "Various different observation spaces can be used with `GridEnv` by passing the `GridObsType` enum when creating the `GridEnv` object. Here we use the `window` observation type, which provides a 5 x 5 local window around the agent as the observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridEnv(obs_type=GridObsType.window)\n",
    "obs = env.reset()\n",
    "plt.imshow(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting dynamic orientation\n",
    "\n",
    "`GridEnv` also supports a dynamic orientation for the agent. This can be selected with the `OrientationType.dynamic` enum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridEnv(orientation_type=OrientationType.variable)\n",
    "obs = env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `dynamic` orientation type, the action space consists of a forward movement action, and 90 degree rotation actions either clockwise or counter clockwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, reward, done, _ = env.step(0)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(2)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training an agent to reach the goal\n",
    "\n",
    "Neuro-Nav includes a number of cannonical RL algorithms. Here we create an agent using the `TDSR` algorithm (Temporal Different Successor Representation), and solve a simple goal-directed navigation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 300\n",
    "num_steps = 100\n",
    "\n",
    "env = GridEnv(\n",
    "    topography=GridTopography.empty,\n",
    "    grid_size=GridSize.small,\n",
    "    obs_type=GridObsType.index,\n",
    "    orientation_type=OrientationType.fixed,\n",
    ")\n",
    "obs = env.reset()\n",
    "env.render()\n",
    "\n",
    "agent = TDSR(\n",
    "    env.state_size,\n",
    "    env.action_space.n,\n",
    "    lr=1e-1,\n",
    "    poltype=\"egreedy\",\n",
    "    epsilon=0.2,\n",
    "    gamma=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps = []\n",
    "for i in range(num_episodes):\n",
    "    obs = env.reset()\n",
    "    agent.reset()\n",
    "    steps = 0\n",
    "    done = False\n",
    "    while not done and steps < num_steps:\n",
    "        act = agent.sample_action(obs)\n",
    "        obs_new, reward, done, _ = env.step(act)\n",
    "        agent.update([obs, act, obs_new, reward, done])\n",
    "        obs = obs_new\n",
    "        steps += 1\n",
    "    total_steps.append(steps)\n",
    "plt.plot(total_steps)\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Time-steps\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neuro-Nav also provides a simple helper function for running episodes. Here we can use the SARSA algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 300\n",
    "num_steps = 100\n",
    "\n",
    "env = GridEnv(\n",
    "    topography=GridTopography.empty,\n",
    "    grid_size=GridSize.micro,\n",
    "    obs_type=GridObsType.index,\n",
    "    orientation_type=OrientationType.fixed,\n",
    ")\n",
    "\n",
    "agent = SARSA(\n",
    "    env.state_size,\n",
    "    env.action_space.n,\n",
    "    lr=1e-1,\n",
    "    poltype=\"egreedy\",\n",
    "    epsilon=0.2,\n",
    "    gamma=0.9,\n",
    ")\n",
    "\n",
    "total_steps = []\n",
    "for i in range(num_episodes):\n",
    "    agent, steps, returns = run_episode(env, agent, max_steps=num_steps)\n",
    "    total_steps.append(steps)\n",
    "plt.plot(total_steps)\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Time-steps\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the reward locations\n",
    "\n",
    "The `objects` argument can be passed to `GridEnv` on reset in order to change the location and value of various objects in the environment, including the reward locations. This takes the form of a dictionary, where the keys are tuples corresponding to the x,y location in the grid, and the values are the rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 300\n",
    "num_steps = 100\n",
    "\n",
    "env = GridEnv(\n",
    "    topography=GridTopography.two_rooms,\n",
    "    grid_size=GridSize.small,\n",
    "    obs_type=GridObsType.index,\n",
    "    orientation_type=OrientationType.fixed,\n",
    ")\n",
    "\n",
    "objects = {\"rewards\": {(2, 2): 1.0, (2, 8): -0.75, (8, 2): -0.75}}\n",
    "\n",
    "obs = env.reset(objects=objects)\n",
    "env.render()\n",
    "\n",
    "agent = TDSR(\n",
    "    env.state_size,\n",
    "    env.action_space.n,\n",
    "    lr=1e-2,\n",
    "    poltype=\"egreedy\",\n",
    "    epsilon=0.2,\n",
    "    gamma=0.9,\n",
    ")\n",
    "\n",
    "total_steps = []\n",
    "for i in range(num_episodes):\n",
    "    agent, steps, _ = run_episode(env, agent, max_steps=num_steps, objects=objects)\n",
    "    total_steps.append(steps)\n",
    "plt.plot(total_steps)\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Time-steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the `GraphEnv`\n",
    "\n",
    "For information on supported environment structures and observation spaces, see [here](https://github.com/awjuliani/neuro-nav/tree/main/neuronav/envs#graphenv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GraphEnv()\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a different graph structure\n",
    "\n",
    "The graph structure can be set with the `GraphStructure` enum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GraphEnv(graph_structure=GraphStructure.two_step)\n",
    "obs = env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training an agent to reach the goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 200\n",
    "num_steps = 25\n",
    "\n",
    "agent = TDQ(env.state_size, env.action_space.n, lr=5e-3, poltype=\"softmax\", beta=1e2)\n",
    "\n",
    "total_returns = []\n",
    "for i in range(num_episodes):\n",
    "    agent, steps, returns = run_episode(env, agent, max_steps=num_steps)\n",
    "    total_returns.append(returns)\n",
    "plt.plot(total_returns)\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Returns\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4416b23800dac0499c77199e6a61cce292c2d7a147d61370d8d704e5424b5c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
